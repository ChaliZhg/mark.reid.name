<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	>

<channel>
	<title>Inductio Ex Machina</title>
	<atom:link href="http://conflate.net/inductio/feed/" rel="self" type="application/rss+xml" />
	<link>http://conflate.net/inductio</link>
	<description>Thoughts on Machine Learning and Inference</description>
	<pubDate>Sat, 19 Jan 2009 00:00:00 +0000</pubDate>
	<generator>http://wordpress.org/?v=2.6</generator>
	<language>en</language>
	<item>
		<title>ML and Stats People on Twitter</title>
		<link>http://conflate.net/inductio/2009/01/16/ml-and-stats-people-on-twitter/</link>
		<comments>http://conflate.net/inductio/2009/01/16/ml-and-stats-people-on-twitter/#comments</comments>
		<pubDate>Fri, 16 Jan 2009 09:25:56 +0000</pubDate>
		<dc:creator>Mark Reid</dc:creator>
				<guid isPermaLink="false">http://conflate.net/inductio/moved</guid>
				<description><![CDATA[Inductio Ex Machina has moved to <a href="http://mark.reid.name/iem">http://mark.reid.name</a>. Please update your feed reader.]]></description>
				<content:encoded><![CDATA[
					<p><i>Inductio Ex Machina</i> has moved to <a href="http://mark.reid.name/iem">http://mark.reid.name</a>.</p>
					<p>No further updates will appear via this RSS stream.</p>
					<p>Please update your feed reader.</p>
				]]></content:encoded>
	</item>		
	<item>
		<title>ML and Stats People on Twitter</title>
		<link>http://conflate.net/inductio/2009/01/16/ml-and-stats-people-on-twitter/</link>
		<comments>http://conflate.net/inductio/2009/01/16/ml-and-stats-people-on-twitter/#comments</comments>
		<pubDate>Fri, 16 Jan 2009 09:25:56 +0000</pubDate>
		<dc:creator>Mark Reid</dc:creator>
		
		<category><![CDATA[Community]]></category>

		<category><![CDATA[twitter]]></category>

		<guid isPermaLink="false">http://conflate.net/inductio/?p=171</guid>
		<description><![CDATA[I started using the social, &#8220;micro-blogging&#8221; service Twitter in February this year simply because I had been seeing so much commentary about it — both good and bad. Since then, I&#8217;ve posted 800+ updates, amassed over 100 followers and follow nearly that many myself.

What has surprised me about Twitter is how many people I have [...]]]></description>
			<content:encoded><![CDATA[<p>I started using the social, &#8220;micro-blogging&#8221; service <a href="http://twitter.com/" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Twitter</a> in February this year simply because I had been seeing so much commentary about it — both good and bad. Since then, I&#8217;ve posted <a href="http://twitter.com/mdreid/" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">800+ updates</a>, amassed over 100 <a href="http://twitter.com/mdreid/followers" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">followers</a> and <a href="http://twitter.com/mdreid/friends" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">follow</a> nearly that many myself.</p>

<p>What has surprised me about Twitter is how many people I have found on there who are active, or at least interested, in machine learning and statistics. The day-to-day discussions, questions, advice and pointers I&#8217;ve got via Twitter have been illuminating and fun.</p>

<p>In an effort to get to know some of these people a bit better I followed the links they provided in their respective profiles to see what they had to say about themselves. The descriptions below are based only on those links as I don&#8217;t find Google-stalking very friendly.</p>

<p>So, in no particular order, here they are:</p>

<h4>Students</h4>

<ul>
<li><p><a href="http://twitter.com/arthegall" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Tim Danford</a><br />
A computer science <a href="http://people.csail.mit.edu/tdanford/" onclick="javascript:urchinTracker ('/outbound/article/people.csail.mit.edu');">Ph.D. student at MIT</a></p></li>
<li><p><a href="http://twitter.com/mja" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Mark James Adams</a><br />
&#8220;<a href="http://affinity.raysend.com/record/about/author" onclick="javascript:urchinTracker ('/outbound/article/affinity.raysend.com');">I am a student of quantitative genetics and a temperamental psychologist</a>&#8220;</p></li>
<li><p><a href="http://twitter.com/dwf" rel="nofollow" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Dave Warde-Farley</a><br />
<a href="http://www.cs.toronto.edu/~dwf/" onclick="javascript:urchinTracker ('/outbound/article/www.cs.toronto.edu');">Computer science Masters student at Toronto</a> working in machine learning</p></li>
<li><p><a href="http://twitter.com/SoloGen" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Amir massoud Farahmand</a><br />
Ph.D. student looking at manifold learning (amongst other things) at the <a href="http://www.cs.ualberta.ca/~amir/" onclick="javascript:urchinTracker ('/outbound/article/www.cs.ualberta.ca');">University of Alberta</a>. Runs the blog <a href="http://thesilog.sologen.net/" onclick="javascript:urchinTracker ('/outbound/article/thesilog.sologen.net');">thesilog</a>.</p></li>
<li><p><a href="http://twitter.com/markusweimer" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Markus Weimer</a><br />
Graduate student working on &#8220;<a href="http://weimo.de/about" onclick="javascript:urchinTracker ('/outbound/article/weimo.de');">applications of machine learning to eLearning</a>&#8220;. Also runs a <a href="http://weimo.de/" onclick="javascript:urchinTracker ('/outbound/article/weimo.de');">blog</a></p></li>
<li><p><a href="http://twitter.com/DataJunkie" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Ryan Rosario</a><br />
Statistics and computer science graduate student.</p></li>
<li><p><a href="http://twitter.com/ansate" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">A.M. Santos</a><br />
Maths and statistics graduate student.</p></li>
</ul>

<h4>Non-students</h4>

<ul>
<li><p><a href="http://twitter.com/nealrichter" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Neal Richter</a><br />
Neal Richter - Runs the blog <a href="http://aicoder.blogspot.com/" onclick="javascript:urchinTracker ('/outbound/article/aicoder.blogspot.com');">aicoder</a></p></li>
<li><p><a href="http://twitter.com/brendan642" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Brendan O&#8217;Connor</a><br />
<a href="http://anyall.org/" onclick="javascript:urchinTracker ('/outbound/article/anyall.org');">Research assistant</a> in NLP at Stanford and consultant at <a href="http://blog.doloreslabs.com/" onclick="javascript:urchinTracker ('/outbound/article/blog.doloreslabs.com');">Dolores Labs</a></p></li>
<li><p><a href="http://twitter.com/dtunkelang" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Daniel Tunkelang</a><br />
Chief scientist at the information retrieval company Endeca and owner of the blog <a href="http://thenoisychannel.com/" onclick="javascript:urchinTracker ('/outbound/article/thenoisychannel.com');">The Noisy Channel</a></p></li>
<li><p><a href="http://twitter.com/ealdent" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Jason Adams</a><br />
Computational linguist work on sentiment analysis. Runs the blog <a href="http://mendicantbug.com/" onclick="javascript:urchinTracker ('/outbound/article/mendicantbug.com');">The Mendicant Bug</a>.</p></li>
<li><p><a href="http://twitter.com/mikiobraun" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Mikio Braun</a><br />
Post-doc at Technische Universität Berlin and a machine learning blogger at <a href="http://mikiobraun.blogspot.com/" onclick="javascript:urchinTracker ('/outbound/article/mikiobraun.blogspot.com');">Marginally Interesting</a>.</p></li>
<li><p><a href="http://twitter.com/lemire" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Daniel Lemire</a><br />
Professor of computer science at the University of Quebec at Montreal and <a href="http://www.daniel-lemire.com/blog/" onclick="javascript:urchinTracker ('/outbound/article/www.daniel-lemire.com');">blogger</a>.</p></li>
<li><p><a href="http://twitter.com/moorejh" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Jason H. Moore</a><br />
Professor of Genetics, Director of Bioinformatics at Dartmouth Medical School. Works on the <a href="http://sourceforge.net/projects/mdr/" onclick="javascript:urchinTracker ('/outbound/article/sourceforge.net');">Multi-factor Dimensionality Reduction</a> software MDR and blogs at <a href="http://compgen.blogspot.com/" onclick="javascript:urchinTracker ('/outbound/article/compgen.blogspot.com');">Epistasis</a>.</p></li>
<li><p><a href="http://twitter.com/peteskomoroch" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Pete Skomoroch</a><br />
Director of analytics at Juice Analytics and <a href="http://www.datawrangling.com/" onclick="javascript:urchinTracker ('/outbound/article/www.datawrangling.com');">Data Wrangling</a> blogger.</p></li>
<li><p><a href="http://twitter.com/smolix" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">Alex Smola</a><br />
Principal Researcher at Yahoo! Research and ex-colleague of mine at <a href="http://nicta.com.au" onclick="javascript:urchinTracker ('/outbound/article/nicta.com.au');">NICTA</a> and the <a href="http://anu.edu.au" onclick="javascript:urchinTracker ('/outbound/article/anu.edu.au');">ANU</a> a.k.a. &#8220;Mr. Kernel&#8221;</p></li>
</ul>

<p>If you are not on this list but think you should be, leave a comment below and I&#8217;ll update this list. Conversely, if I&#8217;ve put you on this list and you don&#8217;t wish to be associated with these sorts of people, leave a comment or send me an email and I&#8217;ll remove you.</p>

<p>Of course, feel free to follow <a href="http://twitter.com/mdreid/" onclick="javascript:urchinTracker ('/outbound/article/twitter.com');">me</a> if you&#8217;d like to keep up with what I&#8217;m doing.</p>
]]></content:encoded>
			<wfw:commentRss>http://conflate.net/inductio/2009/01/16/ml-and-stats-people-on-twitter/feed/</wfw:commentRss>
		</item>
		<item>
		<title>Information, Divergence and Risk for Binary Experiments</title>
		<link>http://conflate.net/inductio/2009/01/06/information-divergence-and-risk/</link>
		<comments>http://conflate.net/inductio/2009/01/06/information-divergence-and-risk/#comments</comments>
		<pubDate>Tue, 06 Jan 2009 09:05:14 +0000</pubDate>
		<dc:creator>Mark Reid</dc:creator>
		
		<category><![CDATA[Theory]]></category>

		<category><![CDATA[Writing]]></category>

		<guid isPermaLink="false">http://conflate.net/inductio/?p=175</guid>
		<description><![CDATA[Bob Williamson and I have finished a report outlining what we have been looking at for the last year or so and uploaded it to the arXiv. Weighing in at 89 pages, it covers a lot of ground in an attempt to unify a number of different classes of measures for problems that can be [...]]]></description>
			<content:encoded><![CDATA[<p><a href="http://axiom.anu.edu.au/~williams/" onclick="javascript:urchinTracker ('/outbound/article/axiom.anu.edu.au');">Bob Williamson</a> and I have finished a <a href="http://arxiv.org/abs/0901.0356" onclick="javascript:urchinTracker ('/outbound/article/arxiv.org');">report</a> outlining what we have been looking at for the last year or so and uploaded it to the arXiv. Weighing in at 89 pages, it covers a lot of ground in an attempt to unify a number of different classes of measures for problems that can be expressed as binary experiments. That is, where instances are drawn from two distributions. This include binary classification, class probability estimation, and hypothesis testing.</p>

<p>We show that many of the usual measures of difficultly for these problems — divergence, information and Bayes risk — are very closely related. We also look at ways in which members of each class of measure can be expressed in terms of &#8220;primitive&#8221; members of those classes. In particular, Fisher-consistent losses (also known as proper scoring rules) can be written as weighted sums of cost-sensitive loss while all f-divergences can be written as weighted sums of something akin to cost-sensitive variational divergence. These &#8220;Choquet representations&#8221; make it easy to derive Pinsker-like bounds for arbitrary f-divergences (not just KL divergence) as well as results similar to those of Bartlett et al in their &#8220;<a href="http://www.citeulike.org/user/mdreid/article/510440" onclick="javascript:urchinTracker ('/outbound/article/www.citeulike.org');">Convexity, classification and Risk Bounds</a>&#8220;.</p>

<p>It should be made clear that many of these results are not new. However, what I like about our approach is that almost all of the results in the paper stem from a two observations about convex functions: they are invariant under the Legendre-Fenchel bidual, and they have a second-order integral Taylor expansion with non-negative weights.</p>

<p>If any of this sounds interesting, you should grab the full paper from the <a href="http://arxiv.org/abs/0901.0356" onclick="javascript:urchinTracker ('/outbound/article/arxiv.org');">arXiv</a>. Here&#8217;s the abstract:</p>

<blockquote>
  <p>We unify f-divergences, Bregman divergences, surrogate loss bounds (regret bounds), 
  proper scoring rules, matching losses, cost curves, ROC-curves and information. We 
  do this by systematically studying integral and variational representations of these 
  objects and in so doing identify their primitives which all are related to cost-sensitive 
  binary classification. As well as clarifying relationships between generative and 
  discriminative views of learning, the new machinery leads to tight and more general 
  surrogate loss bounds and generalised Pinsker inequalities relating f-divergences to 
  variational divergence. The new viewpoint illuminates existing algorithms: it provides a 
  new derivation of Support Vector Machines in terms of divergences and relates 
  Maximum Mean Discrepancy to Fisher Linear Discriminants. It also suggests new 
  techniques for estimating f-divergences.</p>
</blockquote>

<p>Now that we have a good understanding of binary experiments the aim is to build on these results and extend this type of work to other forms of machine learning problems. High on the list are multi-category classification, ranking and regression problems.</p>

<p>Questions, criticism, suggestions and pointers to related work we may have missed are all welcome.</p>
]]></content:encoded>
			<wfw:commentRss>http://conflate.net/inductio/2009/01/06/information-divergence-and-risk/feed/</wfw:commentRss>
		</item>
		<item>
		<title>Machine Learning Summer School 2009</title>
		<link>http://conflate.net/inductio/2008/11/18/machine-learning-summer-school-2009/</link>
		<comments>http://conflate.net/inductio/2008/11/18/machine-learning-summer-school-2009/#comments</comments>
		<pubDate>Tue, 18 Nov 2008 11:18:27 +0000</pubDate>
		<dc:creator>Mark Reid</dc:creator>
		
		<category><![CDATA[Community]]></category>

		<category><![CDATA[Teaching]]></category>

		<category><![CDATA[MLSS]]></category>

		<guid isPermaLink="false">http://conflate.net/inductio/?p=164</guid>
		<description><![CDATA[A plug for the 2009 Machine Learning Summer School in Canberra, Australia. I will be giving a presentation there.]]></description>
			<content:encoded><![CDATA[<p>The annual <a href="http://mlss.cc/" onclick="javascript:urchinTracker ('/outbound/article/mlss.cc');">Machine Learning Summer School</a> is being held in Canberra at the <a href="http://anu.edu.au/" onclick="javascript:urchinTracker ('/outbound/article/anu.edu.au');">Australian National University</a> in January next year. It will be part of the joint <a href="http://ssll.cecs.anu.edu.au/" onclick="javascript:urchinTracker ('/outbound/article/ssll.cecs.anu.edu.au');">Summer Schools in Logic and Learning</a>.</p>

<p>From the 2009 <a href="http://ssll.cecs.anu.edu.au/about/mlss" onclick="javascript:urchinTracker ('/outbound/article/ssll.cecs.anu.edu.au');">MLSS website</a>:</p>

<blockquote>
  <p>This school is suitable for all levels, both for people without previous knowledge in 
  Machine Learning, and those wishing to broaden their expertise in this area. It will 
  allow the participants to get in touch with international experts in this field. 
  Exchange of students, joint publications and joint projects will result because 
  of this collaboration.</p>
</blockquote>

<p>The Summer schools will run from the 26-30th of January 2009 and <a href="http://ssll.cecs.anu.edu.au/registration" onclick="javascript:urchinTracker ('/outbound/article/ssll.cecs.anu.edu.au');">registration</a> is open. Note that there is a 20% surcharge for registrations after the 19th of December 2008 so get registering.</p>

<p>I&#8217;ve been fortunate enough to have been given a spot on the <a href="http://ssll.cecs.anu.edu.au/program" onclick="javascript:urchinTracker ('/outbound/article/ssll.cecs.anu.edu.au');">program</a>. I&#8217;ll be talking about some the work I&#8217;ve been doing with <a href="http://axiom.anu.edu.au/~williams/" onclick="javascript:urchinTracker ('/outbound/article/axiom.anu.edu.au');">Bob Williamson</a> this year on analysing relationships between various notions of risk, divergence and information in binary valued prediction problems.</p>

<p>Leave a comment if you&#8217;re planning to attend and I&#8217;ll make sure I say hi.</p>

<p>Hope to see you there!</p>
]]></content:encoded>
			<wfw:commentRss>http://conflate.net/inductio/2008/11/18/machine-learning-summer-school-2009/feed/</wfw:commentRss>
		</item>
		<item>
		<title>Behold! Jensen&#8217;s Inequality</title>
		<link>http://conflate.net/inductio/2008/11/17/behold-jensens-inequality/</link>
		<comments>http://conflate.net/inductio/2008/11/17/behold-jensens-inequality/#comments</comments>
		<pubDate>Mon, 17 Nov 2008 06:26:15 +0000</pubDate>
		<dc:creator>Mark Reid</dc:creator>
		
		<category><![CDATA[Exposition]]></category>

		<category><![CDATA[convex analysis]]></category>

		<category><![CDATA[maths]]></category>

		<category><![CDATA[proofs]]></category>

		<guid isPermaLink="false">http://conflate.net/inductio/?p=143</guid>
		<description><![CDATA[Unsatisfied with the very algebraic and formal proofs of Jensen's inequality, I present a diagram that gives a graphical intuition for the result.]]></description>
			<content:encoded><![CDATA[<p>I have been making quite a bit of use of Jensen&#8217;s inequality recently. It states that the expected value of a convex transformation of a random variable is at least the value of the convex function at the mean of the random variable. More formally, if ƒ is a real-valued convex function over some finite dimensional convex set <i>X</i> and <i>x</i> is an <i>X</i>-valued random variable then we can define the <em>Jensen gap</em>
<center>
<img src='/inductio/wp-content/plugins/latexrender/pictures/9435410b90665f6610231fea660aa7fa_3.86108pt.png' title='&#13;&#10;\displaystyle J_f(x) := \mathbb{E}\left[ f\left(x\right) \right] - f\left(\mathbb{E}\left[ x \right]\right)&#13;&#10;' alt='&#13;&#10;\displaystyle J_f(x) := \mathbb{E}\left[ f\left(x\right) \right] - f\left(\mathbb{E}\left[ x \right]\right)&#13;&#10;'  style="vertical-align:-3.86108pt;" >
</center>
where <img src='/inductio/wp-content/plugins/latexrender/pictures/fecec5b6d5e9ca2d939f47d15bb9f56e_1.0pt.png' title='\mathbb{E}' alt='\mathbb{E}'  style="vertical-align:-1.0pt;" > denotes expectation. Jensen&#8217;s inequality states that this gap is never negative, that is, <img src='/inductio/wp-content/plugins/latexrender/pictures/3e79fe855e4ba0438377ac57e2b06b21_3.86108pt.png' title='J_f(x) \geq 0' alt='J_f(x) \geq 0'  style="vertical-align:-3.86108pt;" > or equivalently,
<center>
<img src='/inductio/wp-content/plugins/latexrender/pictures/cad710481cb4b13a4e09a2d016a2e0c8_3.5pt.png' title='&#13;&#10;\displaystyle \mathbb{E}\left[ f\left(x\right) \right] \geq f\left(\mathbb{E}\left[ x \right]\right).&#13;&#10;' alt='&#13;&#10;\displaystyle \mathbb{E}\left[ f\left(x\right) \right] \geq f\left(\mathbb{E}\left[ x \right]\right).&#13;&#10;'  style="vertical-align:-3.5pt;" >
</center></p>

<p>This is a fairly simple but important inequality in the study of convex functions. Through judicious choice of the convex function it can be used to derive a <a href="http://en.wikipedia.org/wiki/Inequality_of_arithmetic_and_geometric_means#Proof_of_the_generalized_AM-GM_inequality_using_Jensen.27s_inequality" onclick="javascript:urchinTracker ('/outbound/article/en.wikipedia.org');">general AM-GM inequality</a> and many results in information theory. I&#8217;ve been interested in it because DeGroot&#8217;s notion of <a href="http://projecteuclid.org/euclid.aoms/1177704567" onclick="javascript:urchinTracker ('/outbound/article/projecteuclid.org');">statistical information</a> and measures of the distance between probability distributions called <a href="http://en.wikipedia.org/wiki/F-divergence" onclick="javascript:urchinTracker ('/outbound/article/en.wikipedia.org');">f-divergences</a> can both be expressed as a Jensen gap and consequently related to each other.</p>

<p>Jensen&#8217;s inequality is not difficult to prove. It is almost a direct consequence of the definition of convexity and the linearity of expectation. However, all of the proofs I&#8217;ve read, including those in books by <a href="http://books.google.com/books?id=wj4Fh4h_V7QC" onclick="javascript:urchinTracker ('/outbound/article/books.google.com');">Rockafellar</a> and by <a href="http://books.google.com/books?id=Wv_zxEExK3QC" onclick="javascript:urchinTracker ('/outbound/article/books.google.com');">Dudley</a> feel like they are from the <a href="http://en.wikipedia.org/wiki/Nicolas_Bourbaki" onclick="javascript:urchinTracker ('/outbound/article/en.wikipedia.org');">Bourbaki</a> school in that they present the proof without recourse to any diagrams.</p>

<p>I was quite happy then to have found a graphical &#8220;proof&#8221; of Jensen&#8217;s inequality. By this I mean a proof in the style of the <a href="http://www.math.ntnu.no/~hanche/pythagoras/" onclick="javascript:urchinTracker ('/outbound/article/www.math.ntnu.no');">proof of Pythagoras&#8217; theorem</a> that is simply a diagram with the word &#8220;Behold!&#8221; above it.</p>

<p><div id="attachment_154" class="wp-caption aligncenter" style="width: 495px"><a href="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png" ><img src="http://conflate.net/inductio/wp-content/uploads/2008/11/jensen.png" alt="Jensen\&#039;s Inequality" title="Jensen\&#039;s Inequality" width="485" height="420" class="size-full wp-image-154" /></a><p class="wp-caption-text">Figure 1. Behold! A graphical demonstration of Jensen&#039;s Inequality. The expectations shown are with respect to an arbitrary discrete distribution over the x<sub>i</sub></p></div></p>

<p>Unfortunately, the diagram in Figure 1 is not quite as transparent as the Pythagorean proof so a little discussion is probably required. The diagram shows an instance of Jensen&#8217;s inequality for a discrete distribution where the random variable <img src='/inductio/wp-content/plugins/latexrender/pictures/9dd4e461268c8034f5c8564e155c67a6_1.0pt.png' title='x' alt='x'  style="vertical-align:-1.0pt;" > takes on one of the <i>n</i> values <img src='/inductio/wp-content/plugins/latexrender/pictures/1ba8aaab47179b3d3e24b0ccea9f4e30_2.49998pt.png' title='x_i' alt='x_i'  style="vertical-align:-2.49998pt;" > with with probability <img src='/inductio/wp-content/plugins/latexrender/pictures/eca91c83a74a2373ca5f796700e99fd3_2.94444pt.png' title='p_i' alt='p_i'  style="vertical-align:-2.94444pt;" >.</p>

<p>Note that the points <img src='/inductio/wp-content/plugins/latexrender/pictures/c52bdcfaee9da6804d8f290862f2472d_3.5pt.png' title='(x_i, f(x_i))' alt='(x_i, f(x_i))'  style="vertical-align:-3.5pt;" > form the vertices of a polygon which, by the convexity of ƒ, must also be convex and lie within the epigraph of ƒ (the blue shaded area above ƒ). Furthermore, since the <img src='/inductio/wp-content/plugins/latexrender/pictures/eca91c83a74a2373ca5f796700e99fd3_2.94444pt.png' title='p_i' alt='p_i'  style="vertical-align:-2.94444pt;" > are probabilities they satisfy <img src='/inductio/wp-content/plugins/latexrender/pictures/a0cf901dcd86dba03e24e0d68963640e_4.00005pt.png' title='\sum_i p_i = 1' alt='\sum_i p_i = 1'  style="vertical-align:-4.00005pt;" >. This means the expected value of the random variable <img src='/inductio/wp-content/plugins/latexrender/pictures/af2c2482e5d22714e9854d6a79b815de_3.5pt.png' title='(x, f(x))' alt='(x, f(x))'  style="vertical-align:-3.5pt;" > given by
<center>
<img src='/inductio/wp-content/plugins/latexrender/pictures/8a2771902f90c3f1cdde6b5c4521b46f_13.79865pt.png' title='\displaystyle &#13;&#10;   \mathbb{E}[(x, f(x))] = \sum_{i=1}^n p_i \left(x_i, f(x_i)\right) &#13;&#10;' alt='\displaystyle &#13;&#10;   \mathbb{E}[(x, f(x))] = \sum_{i=1}^n p_i \left(x_i, f(x_i)\right) &#13;&#10;'  style="vertical-align:-13.79865pt;" >
</center>
is a convex combination and so must also lie within the dashed polygon. In fact, since <img src='/inductio/wp-content/plugins/latexrender/pictures/42bb32898bf339f66f3241b34e8ef889_3.5pt.png' title='\mathbb{E}[(x, f(x))] = \left(\mathbb{E}[x], \mathbb{E}[f(x)]\right)' alt='\mathbb{E}[(x, f(x))] = \left(\mathbb{E}[x], \mathbb{E}[f(x)]\right)'  style="vertical-align:-3.5pt;" > it must lie above <img src='/inductio/wp-content/plugins/latexrender/pictures/054a2a3e4ca71d7c5bfc0697e2267519_3.5pt.png' title='f\left(\mathbb{E}[x]\right)' alt='f\left(\mathbb{E}[x]\right)'  style="vertical-align:-3.5pt;" > thus giving the result.</p>

<p>Although the diagram in Figure 1 assumes a 1-dimensional space <i>X</i> the above argument generalises to higher dimensions in an analogous manner. Also, the general result for non-discrete distributions can be gleamed from the provided diagram by a hand-wavy limiting argument. By adding more <img src='/inductio/wp-content/plugins/latexrender/pictures/1ba8aaab47179b3d3e24b0ccea9f4e30_2.49998pt.png' title='x_i' alt='x_i'  style="vertical-align:-2.49998pt;" > to the diagram the dashed polygon the shaded area will approximate the graph of ƒ better. So, by the earlier argument for the discrete case, the expected value of <img src='/inductio/wp-content/plugins/latexrender/pictures/9dd4e461268c8034f5c8564e155c67a6_1.0pt.png' title='x' alt='x'  style="vertical-align:-1.0pt;" > will remain within the polygon and thus within the shaded area and thus above ƒ. Since this holds for an arbitrary number of points and nothing weird happens as we take the limit we have the continuous result.</p>

<p>A somewhat surprising fact about Jensen&#8217;s inequality is that its converse is also true. By this I mean that if ƒ is a function such that its Jensen gap <img src='/inductio/wp-content/plugins/latexrender/pictures/3f31503c6b4da81bf3006024075ffe96_3.86108pt.png' title='J_f(x)' alt='J_f(x)'  style="vertical-align:-3.86108pt;" > is non-negative for all distributions of the random variable <i>x</i> then ƒ is necessarily convex. The contrapositive of this statement is: ƒ non-convex implies the existence of a random variable <i>x</i> so that <img src='/inductio/wp-content/plugins/latexrender/pictures/a2fd5563cfeaf43bb8b9f1e9eb69efaa_3.86108pt.png' title='J_f(x) &lt; 0' alt='J_f(x) &lt; 0'  style="vertical-align:-3.86108pt;" >.</p>

<p>Considering Figure 1 again gives some intuition as to why this must be the case. If ƒ was non-convex then its epigraph must, by definition, also be non-convex. This means I could choose some <img src='/inductio/wp-content/plugins/latexrender/pictures/1ba8aaab47179b3d3e24b0ccea9f4e30_2.49998pt.png' title='x_i' alt='x_i'  style="vertical-align:-2.49998pt;" > so that one of the dashed lines lies outside the shaded area. This means I can then choose <img src='/inductio/wp-content/plugins/latexrender/pictures/eca91c83a74a2373ca5f796700e99fd3_2.94444pt.png' title='p_i' alt='p_i'  style="vertical-align:-2.94444pt;" > so that the mean <img src='/inductio/wp-content/plugins/latexrender/pictures/ddda728c1d80815d9ae197466d381843_3.5pt.png' title='\mathbb{E}[(x, f(x))]' alt='\mathbb{E}[(x, f(x))]'  style="vertical-align:-3.5pt;" > lies outside the shaded area and thus below the graph of ƒ.</p>

<p>Of course, no self-respecting mathematician would call the above arguments a proof of Jensen&#8217;s inequality. There are too many edge cases and subtleties (especially in the continuous case) that I&#8217;ve ignored. That said, I believe the statement and thrust of the inequality can be quickly arrived at from the simple diagram above. When using tools like Jensen&#8217;s inequality, I find this type of quick insight more valuable than a long, careful technical statement and proof. The latter is valuable to but if I need this level of detail I would look it up rather than try to dredge it up from my sometimes unreliable memory.</p>
]]></content:encoded>
			<wfw:commentRss>http://conflate.net/inductio/2008/11/17/behold-jensens-inequality/feed/</wfw:commentRss>
		</item>
		<item>
		<title>Artificial AI</title>
		<link>http://conflate.net/inductio/2008/11/07/artificial-ai/</link>
		<comments>http://conflate.net/inductio/2008/11/07/artificial-ai/#comments</comments>
		<pubDate>Fri, 07 Nov 2008 01:52:41 +0000</pubDate>
		<dc:creator>Mark Reid</dc:creator>
		
		<category><![CDATA[General]]></category>

		<guid isPermaLink="false">http://conflate.net/inductio/?p=140</guid>
		<description><![CDATA[Anyone who has working in the area for long enough knows how difficult creating any type of artificial intelligence can be. Like many before me, I&#8217;ve decided to cheat a little and create an artificial AI. I take partial credit for the initial idea but it is my wife, Julieanne, who has been responsible for [...]]]></description>
			<content:encoded><![CDATA[<p>Anyone who has working in the area for long enough knows how difficult creating any type of artificial intelligence can be. Like many before me, I&#8217;ve decided to cheat a little and create an artificial AI. I take partial credit for the initial idea but it is my wife, Julieanne, who has been responsible for most of the development over the last nine months.</p>

<p>We had our official release on the 26th of October and even though she&#8217;s been here for less than two weeks she is already exceeding all our expectations.</p>

<p>We refer to her as &#8220;Ada Molly Reid&#8221;.</p>

<p><center>
<a href="http://conflate.net/inductio/wp-content/uploads/2008/11/ada.jpg" ><img src="http://conflate.net/inductio/wp-content/uploads/2008/11/ada.jpg" alt="An Artificial AI, completed on the 26th of October, 2008." title="Ada Molly Reid" width="170" height="130" class="size-full wp-image-141" /></a></center></p>
]]></content:encoded>
			<wfw:commentRss>http://conflate.net/inductio/2008/11/07/artificial-ai/feed/</wfw:commentRss>
		</item>
	</channel>
</rss>
